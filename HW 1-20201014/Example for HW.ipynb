{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook will contain a lot of comments that hopefully helps you understand things better. \n",
    "\n",
    "**Tip:** I cannot stress this enough, if you are not sure how any function/class/method/concept works there is usually a brief example demonstrating a use case. Then you can copy it and mess around with it. For example use `dir(<object>)` to print its attributes and methods and `type(<object>)` for its type and `print(<object>)` to print its value/contents. This is, in fact, how you learn libraries; by directly interacting with them to see what you can use and how. While you do that you will inevitably encounter errors. **Read what the error says** they are usually informative for deducing where the code went wrong. If you can't figure it out, search for that error online. There is likely many other people that have gone the same path and made the same mistake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code to get you started\n",
    "\n",
    "Let's first create some data. We will use the random package of numpy which let's us generate pseudo-random data from many different types of probability distributions. In this case we will be using normal (Gaussian) distribution in multiple dimensions (multivariate normal). The `mean` parameter denotes the mean of the distribution and `cov` the covariance matrix. If `mean` is N dimensional, `cov` must be NxN. Furthermore, cov must be a \"symmetric positive semi-definite matrix\" (you can read what PSD means on wikipedia). For our purposes it suffices to say that if we take the outer product of a vector with itself we get a symmetric PSD matrix.\n",
    "\n",
    "We will be generating two distributions with different means. Dataset A will have a mean \\[1,1,1\\] and covariance $X_1^TX_1$ and Dataset B will have a mean \\[2,1,3\\] and covariance $X_2^TX_2$. There is no particular reason we use `np.random.rand` for $X_2$ I just wanted to show that `numpy.random.rand` is another distribution. It generates an array (of size given in parantheses) from the uniform distribution $\\mathcal{U}[0,1]$. For reference, `@` performs a matrix multiplication for 2 dimensional arrays in Python, provided that the dimensions match. And double slash `//` does integer division (like integer divisions in C). So the last two rows generate dataset A and dataset B with the given means, covariances are calculated by outer product $X^TX$ and there will be N/2 many points in each dataset denoted by size. \n",
    "\n",
    "Digression: if you are wondering how to write code blocks or bold or italic in Jupyter notebooks, search \"Markdown syntax\" on Google. If you want to use mathematical notation, look up \"LaTeX syntax\". There are many good entry level tutorials online for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N=500\n",
    "\n",
    "X1=np.array([[0.3,-0.2,0.3],[0.3,0.3,0.2],[0.1,0.4,0.5]])\n",
    "X2=np.random.rand(3,3)\n",
    "setA=np.random.multivariate_normal(mean=[1,1,1], cov=X1.T@X1, size=N//2)\n",
    "setB=np.random.multivariate_normal(mean=[2,1,2], cov=X2.T@X2, size=N//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch out**: some functions (like `np.random.rand` above) take the dimensions *as regular arguments* and others like `np.zeros` or `np.ones` take the dimensions *in a tuple*. Ex: `np.zeros((5,5))`. If you're unsure which one it is, consult the docs of that package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"use array[a:b,c:d] to print elements in the array's rows a through b (b not inclusive) and columns c through d (d not inclusive): \\n\" + str(setA[4:6,0:2]))\n",
    "print(\"If you omit the first index it will take it to be 0. If you omit the last index, it will take it to be -1 (meaning last index): \\n\" + str(setA[:6,1:]))\n",
    "print(\"If you omit both, it will print all elements in that index: \\n\" + str(setA[245:,:]))\n",
    "print(\"You can also use negative integers to count backwards from last: \\n\" + str(setA[-5:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default ending for print is \\n. You can change it:\n",
    "print(\"Be careful, if you index with a single number, that dimension is squeezed out: \" + str(setA[1:2,:]), end=\"\")\n",
    "print(\" vs \" + str(setA[1,:]))\n",
    "# f-string example:\n",
    "print(f\"{setA[:,1:2].shape} and {setA[:,1].shape} are not the same shapes even though they contain the same elements.\\nThis might be a problem for example when you are trying to concatenate arrays.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if you are running jupyter notebook in the browser\n",
    "%matplotlib notebook\n",
    "\n",
    "# if you are running jupyter notebook in vscode\n",
    "# %matplotlib widget\n",
    "\n",
    "# this creates a new figure with the given figsize\n",
    "# .gca stands for get current axes. without calling that with\n",
    "# projection='3d' matplotlib assumes the plot is 2d\n",
    "# don't worry too much about this\n",
    "fig=plt.figure(figsize=(5,5)).gca(projection='3d')\n",
    "\n",
    "# scatter takes in x,y,z values. Obtained by indexing different columns\n",
    "fig.scatter(setA[:,0],setA[:,1],setA[:,2]);\n",
    "fig.scatter(setB[:,0],setB[:,1],setB[:,2]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example here is a 2d projection of the same data on the first two axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without calling the gca(projection='3d') the default is 2d\n",
    "plt.figure(figsize=(5,5))\n",
    "# you can set facecolors, edgecolors, thickness etc\n",
    "# matplotlib is very flexible\n",
    "plt.scatter(setA[:,0],setA[:,1], edgecolor =(0.,0.4,0.4), facecolor=\"xkcd:turquoise\");\n",
    "plt.scatter(setB[:,0],setB[:,1], edgecolor =(0.5,0.,0.7,0.3), facecolor=(0.5,0.,0.5,0.4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are concatenating all zeros to `setA` and all ones to `setB` to denote their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how concatenate takes the arrays to be concatenated in a tuple\n",
    "# also see the dimensions of np.zeros are in a tuple\n",
    "# that's a lot of parantheses!\n",
    "setA=np.concatenate((setA,np.zeros((N//2,1))), axis=1)\n",
    "setB=np.concatenate((setB,np.ones((N//2,1))), axis=1)\n",
    "print(setB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then concatenate setA and setB to create a mixed dataset and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset=np.concatenate((setA,setB),axis=0)\n",
    "np.random.shuffle(total_dataset)\n",
    "print(total_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's `train_test_split` function gives a convenient way of splitting the data (we could have just as well done this by indexing). Pay attention: we will use test_dataset only once, after we fine tune the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_validation_dataset,test_dataset=train_test_split(total_dataset,test_size=0.2)\n",
    "length=len(train_validation_dataset)\n",
    "\n",
    "# int() how you type cast in python\n",
    "train_dataset=train_validation_dataset[:int(length*7/8)]\n",
    "validation_dataset=train_validation_dataset[int(length*7/8):]\n",
    "\n",
    "\n",
    "X_train_validation,y_train_validation=train_validation_dataset[:,:-1],train_validation_dataset[:,-1]\n",
    "X_train,y_train=train_dataset[:,:-1],train_dataset[:,-1]\n",
    "X_validation,y_validation=validation_dataset[:,:-1],validation_dataset[:,-1]\n",
    "X_test,y_test=test_dataset[:,:-1],test_dataset[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For our purposes there are a few basic *tasks* in data science: regression, classification, clustering, dimensionality reduction etc. (See [here](https://scikit-learn.org/stable/user_guide.html) for various tasks you can use scikit-learn in). In this particular case, we are aiming to trying to tell these two datasets apart i.e. classify them. One of the ways to classify data is using decision trees. To achieve this, we are going to use `DecisionTreeClassifier` *estimator* from `scikit-learn` package.\n",
    "\n",
    "There are a few basic methods of various estimators that you should pay special attention to:\n",
    "\n",
    "`fit()`: fits i.e. trains the estimator (also called model sometimes) on the train data and train labels (if labels are available and required by the estimator) Ex: `DecisionTreeClassifier.fit()`, `LinearRegression.fit()`, `KNeighborsClassifier.fit()`\n",
    "\n",
    "`predict()`: gives its predictions for validation/test data (does not take validation/test labels!) Ex: `KNeighborsClassifier.predict()`, `SVC.predict()`, `SGDRegressor.predict()`\n",
    "\n",
    "`fit_predict()`: some of the estimators both fit and predict at the same time (the ones that don't require labels such as clustering estimators) Ex: `SpectralClustering.fit_predict()`, `AgglomerativeClustering.fit_predict()`\n",
    "\n",
    "`transform()`: some of the estimators don't predict but rather *transform* the data, such as dimensionality reduction, projection or preprocessing estimators. Ex: `GaussianRandomProjection.transform()`, `SparseRandomProjection.transform()`, `PCA.transform()`\n",
    "\n",
    "`fit_transform()`: for the estimators that `transform`, `fit_transform` combines `fit` and `transform` into one operation. Ex: `GaussianRandomProjection.fit_transform()`, `SparseRandomProjection.fit_transform()`, `PCA.fit_transform()`\n",
    "\n",
    "If you're confused about how a random projection or PCA can `fit` the data and then `transform` it see [this link](https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt=DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
    "\n",
    "# fit() i.e. train the decision tree classifier on the train data\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# predict() i.e. get the predictions from the trained classifier on validation data\n",
    "y_validation_pred=dt.predict(X_validation)\n",
    "\n",
    "print(f\"Validation accuracy: {accuracy_score(y_validation,y_validation_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing with different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dt.set_params(criterion='gini', max_depth=10)\n",
    "\n",
    "# fit() i.e. train the decision tree classifier on the train data\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# predict() i.e. get the predictions from the trained classifier on validation data\n",
    "y_validation_pred=dt.predict(X_validation)\n",
    "\n",
    "print(f\"Validation accuracy: {accuracy_score(y_validation,y_validation_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. it seems that these are better hyperparameters. Let's go with the second set of hyperparameters. Now let's use the entire `train_validation_dataset` to train the classifier and test it on `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is redundant. Since the last set of hyperparameters \n",
    "# applied were these, they haven't changed.\n",
    "dt=dt.set_params(criterion='gini', max_depth=10)\n",
    "\n",
    "# use all train+validation\n",
    "dt.fit(X_train_validation,y_train_validation)\n",
    "\n",
    "y_test_pred=dt.predict(X_test)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy_score(y_test,y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see how each individual datapoint is classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip() takes in two arrays and goes through them simultaneously\n",
    "# it is used instead of \n",
    "# for i in range(10):\n",
    "#    array1[i] ... do something\n",
    "#    array2[i] ... do something\n",
    "for predict,label in zip(y_test_pred, y_test):\n",
    "    print(\"Predicted class: \" +str(predict) + f\", true class: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Grid Search\n",
    "\n",
    "Now in the above example we have not used cross validation (CV) nor did we test many different hyperparameter combinations. To discover the best set of hyperparameters, we can use grid search. This basically means training your model for different combinations of hyperparameters (going over them systematically) and choosing the best one. Luckily for us, there is a function in `scikit-learn` that combines both CV and grid search in one entity. It is called `GridSearchCV`. What this does is it wraps around your estimator and creates another estimator object. While creating this new estimator you specify how many folds you want in your cross validation as well as the parameters to search over (in terms of a Python dictionary). Then, when you call `fit` on this newly created wrapper, it will take care of the grid search and cross validation internally. \n",
    "\n",
    "**Tip**: To see an estimator's hyperparameters, you can go to its docs on `scikit-learn` website.\n",
    "\n",
    "Let's do an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "# I want to know how many seconds grid search takes\n",
    "# time() returns a timestamp in terms of seconds\n",
    "start=time()\n",
    "\n",
    "# it doesn't matter if we give initial hyperparameters to DecisionTreeClassifier\n",
    "# they will be overriden by GridSearchCV anyway\n",
    "dt=DecisionTreeClassifier()\n",
    "\n",
    "# see how we specify cv=10 and param_grid as a Python dictionary\n",
    "# GridSearchCV will go through all combinations of these hyperparameters and find the best one for us\n",
    "# Make sure the values of the dictionary are Python tuples, lists or numpy arrays\n",
    "wrapper=GridSearchCV(estimator=dt, cv=10, param_grid={\"criterion\":('gini','entropy'), \"max_depth\":np.linspace(1,10,10)})\n",
    "\n",
    "# we use the whole train_validation_dataset because GridSearchCV does the validation split internally\n",
    "wrapper.fit(X_train_validation,y_train_validation)\n",
    "\n",
    "stop=time()\n",
    "print(\"Time elapsed: \" + str(stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all results pertaining to the CV and grid search in `.cv_results_` attribute. This includes the 10 splits' accuracies for all hyperparameter combinations (see `split0_test_score`, `split1_test_score` etc) and mean scores (averaged over different splits) (see `mean_test_score`), as well as the list of all parameter combinations (see `params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best score (accuracy) and the hyperparameter combination that achieves the best score like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters are:\", wrapper.best_params_)\n",
    "print(\"Best accuracy is:\", wrapper.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even directly get the best estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=wrapper.best_estimator_\n",
    "\n",
    "# an alternative could be\n",
    "# dt.set_params(**wrapper.best_params_)\n",
    "# or \n",
    "# dt=DecisionTreeClassifier(**wrapper.best_params_)\n",
    "\n",
    "# FYI: double asterisk ** is used to expand dictionaries as if they are being used as separate arguments.\n",
    "# For example: \n",
    "# dictionary={\"a\":1, \"b\":2}\n",
    "# myfunc(**dictionary) is equivalent to myfunc(a=1, b=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the decision tree with the best hyperparameters using `train_validation_dataset` and test on `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train_validation, y_train_validation)\n",
    "y_test_pred=dt.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to `GridSearchCV` is to use `StratifiedKFold` and `ParameterGrid`. These are used to implement K-fold CV and grid search respectively. `StratifiedKFold` gives us the indices of train and validation splits so we can use them in a for loop. `ParameterGrid` on the other hand generates the Cartesian product of all the parameter lists it is given. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# create a StratifiedKfold object\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "param_options = {\"criterion\":('gini','entropy'), \"max_depth\":np.linspace(1,10,10)}\n",
    "param_grid=ParameterGrid(param_options) # this will give an iterator that iteratos over all possible hyperparameter combinations\n",
    "print(param_grid[0])\n",
    "print(param_grid[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use two nested for loops: one that cycles through the hyperparameter combinations, the other cycles through different validation/train data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create an array of zeros to keep mean_accuracies (averaged over splits) for each hyperparameter combination\n",
    "mean_accuracies=np.zeros(len(param_grid))\n",
    "\n",
    "# measure time\n",
    "start=time()\n",
    "\n",
    "for i,param_dict in enumerate(param_grid):\n",
    "    # create an empty array to keep accuracies for different CV folds\n",
    "    fold_accuracies=[]\n",
    "\n",
    "    # set the hyperparameters to current iteration's combination\n",
    "    dt.set_params(**param_dict)\n",
    "\n",
    "    for train_index, validation_index in skf.split(X_train_validation,y_train_validation):\n",
    "        # skf gave us train and validation indices. use them\n",
    "        X_train, X_validation = X_train_validation[train_index], X_train_validation[validation_index]\n",
    "        y_train, y_validation = y_train_validation[train_index], y_train_validation[validation_index]\n",
    "\n",
    "        # train the model with these hyperparameters and data\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_predict=dt.predict(X_validation)\n",
    "\n",
    "        # append current fold's accuracy to fold_accuracies\n",
    "        fold_accuracies.append(accuracy_score(y_validation, y_predict))\n",
    "\n",
    "    # record the accuracy averaged across folds to mean_accuracies \n",
    "    mean_accuracies[i]=np.mean(fold_accuracies)\n",
    "    \n",
    "stop=time()\n",
    "\n",
    "print(\"Time: \"+ str(stop-start))\n",
    "print(\"Mean accuracies: \"+str(mean_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index=np.argmax(mean_accuracies)\n",
    "print(\"Max accuracy: \"+str(mean_accuracies[max_index]))\n",
    "optimal_hyperparameters=param_grid[max_index]\n",
    "print(\"Hyperparameters achieving max accuracy: \" +str(optimal_hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.set_params(**optimal_hyperparameters)\n",
    "dt.fit(X_train,y_train)\n",
    "y_test_predict=dt.predict(X_test)\n",
    "print(accuracy_score(y_test, y_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
